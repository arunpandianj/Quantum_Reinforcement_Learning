{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Quantum Policy Gradient RL Example\n", "This notebook demonstrates a simple 1D grid environment using a quantum variational circuit as a policy network."]}, {"cell_type": "code", "metadata": {}, "source": ["import pennylane as qml\n", "from pennylane import numpy as np\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Environment Definition\n", "We create a simple 1D grid environment where the agent starts at 0 and the goal is at the last cell."]}, {"cell_type": "code", "metadata": {}, "source": ["class SimpleGridEnv:\n", "    def __init__(self, size=5):\n", "        self.size = size\n", "        self.state = 0\n", "        self.goal = size-1\n", "\n", "    def reset(self):\n", "        self.state = 0\n", "        return self.state\n", "\n", "    def step(self, action):\n", "        self.state = max(0, min(self.size-1, self.state + (1 if action else -1)))\n", "        reward = 1 if self.state == self.goal else 0\n", "        done = self.state == self.goal\n", "        return self.state, reward, done"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Quantum Policy Network\n", "Using a 2-qubit variational circuit as the policy to decide actions."]}, {"cell_type": "code", "metadata": {}, "source": ["n_qubits = 2\n", "dev = qml.device(\"default.qubit\", wires=n_qubits)\n", "\n", "def variational_circuit(state, params):\n", "    for i in range(n_qubits):\n", "        qml.RY(state + params[i], wires=i)\n", "        qml.RZ(params[i+n_qubits], wires=i)\n", "    qml.CNOT(wires=[0,1])\n", "\n", "@qml.qnode(dev, interface=\"autograd\")\n", "def circuit(state, params):\n", "    variational_circuit(state, params)\n", "    return [qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))]\n", "\n", "def get_action_probs(state, params):\n", "    state = float(state)\n", "    output = circuit(state, params)\n", "    probs = (np.array(output) + 1)/2\n", "    probs = probs / np.sum(probs)\n", "    return probs\n", "\n", "def select_action(state, params):\n", "    probs = get_action_probs(state, params)\n", "    return int(np.random.choice([0,1], p=probs))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Training (Policy Gradient)"]}, {"cell_type": "code", "metadata": {}, "source": ["env = SimpleGridEnv(size=5)\n", "params = 0.01 * np.random.randn(2*n_qubits)\n", "lr = 0.1\n", "episodes = 50\n", "\n", "for ep in range(episodes):\n", "    state = env.reset()\n", "    done = False\n", "    while not done:\n", "        action = select_action(state, params)\n", "        next_state, reward, done = env.step(action)\n", "        \n", "        def loss(p):\n", "            probs = get_action_probs(state, p)\n", "            return -np.log(probs[action]) * reward\n", "\n", "        grads = qml.grad(loss)(params)\n", "        params = params - lr * grads\n", "        state = next_state\n", "\n", "print(\"Trained parameters:\", params)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Evaluation"]}, {"cell_type": "code", "metadata": {}, "source": ["eval_episodes = 5\n", "for ep in range(eval_episodes):\n", "    state = env.reset()\n", "    done = False\n", "    path = [state]\n", "    while not done:\n", "        action = select_action(state, params)\n", "        state, _, done = env.step(action)\n", "        path.append(state)\n", "    print(f\"Episode {ep+1} path: {path}\")\n", "    \n", "    plt.figure()\n", "    plt.plot(path, 'o-', markersize=12)\n", "    plt.xticks(range(env.size))\n", "    plt.yticks([])\n", "    plt.title(f\"Episode {ep+1} Path\")\n", "    plt.xlabel(\"Steps\")\n", "    plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}