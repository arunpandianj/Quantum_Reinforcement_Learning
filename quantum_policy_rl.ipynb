{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x25KTHz5yI3k"
      },
      "source": [
        "# Quantum Policy Gradient RL Example\n",
        "This notebook demonstrates a simple 1D grid environment using a quantum variational circuit as a policy network."
      ],
      "id": "x25KTHz5yI3k"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane pennylane-qiskit qiskit matplotlib"
      ],
      "metadata": {
        "id": "lT5nfHPCyRMy"
      },
      "id": "lT5nfHPCyRMy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbh3MrX_yI3n"
      },
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "Jbh3MrX_yI3n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ewvEP-ZyI3o"
      },
      "source": [
        "## 1. Environment Definition\n",
        "We create a simple 1D grid environment where the agent starts at 0 and the goal is at the last cell."
      ],
      "id": "-ewvEP-ZyI3o"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rD5qslVyI3o"
      },
      "source": [
        "class SimpleGridEnv:\n",
        "    def __init__(self, size=5):\n",
        "        self.size = size\n",
        "        self.state = 0\n",
        "        self.goal = size-1\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        self.state = max(0, min(self.size-1, self.state + (1 if action else -1)))\n",
        "        reward = 1 if self.state == self.goal else 0\n",
        "        done = self.state == self.goal\n",
        "        return self.state, reward, done"
      ],
      "id": "4rD5qslVyI3o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWnUKH5wyI3o"
      },
      "source": [
        "## 2. Quantum Policy Network\n",
        "Using a 2-qubit variational circuit as the policy to decide actions."
      ],
      "id": "uWnUKH5wyI3o"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L659vrKyI3q"
      },
      "source": [
        "n_qubits = 2\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "def variational_circuit(state, params):\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(state + params[i], wires=i)\n",
        "        qml.RZ(params[i+n_qubits], wires=i)\n",
        "    qml.CNOT(wires=[0,1])\n",
        "\n",
        "@qml.qnode(dev, interface=\"autograd\")\n",
        "def circuit(state, params):\n",
        "    variational_circuit(state, params)\n",
        "    return [qml.expval(qml.PauliZ(0)), qml.expval(qml.PauliZ(1))]\n",
        "\n",
        "def get_action_probs(state, params):\n",
        "    state = float(state)\n",
        "    output = circuit(state, params)\n",
        "    probs = (np.array(output) + 1)/2\n",
        "    probs = probs / np.sum(probs)\n",
        "    return probs\n",
        "\n",
        "def select_action(state, params):\n",
        "    probs = get_action_probs(state, params)\n",
        "    return int(np.random.choice([0,1], p=probs))"
      ],
      "id": "4L659vrKyI3q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX-AgGZuyI3q"
      },
      "source": [
        "## 3. Training (Policy Gradient)"
      ],
      "id": "qX-AgGZuyI3q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_zJ-5zDyI3r"
      },
      "source": [
        "env = SimpleGridEnv(size=5)\n",
        "params = 0.01 * np.random.randn(2*n_qubits)  # 4 parameters\n",
        "lr = 0.1\n",
        "episodes = 50\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = select_action(state, params)\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        # Policy gradient loss\n",
        "        def loss(p):\n",
        "            probs = get_action_probs(state, p)\n",
        "            return -np.log(probs[action]) * reward\n",
        "\n",
        "        grads = qml.grad(loss)(params)\n",
        "        params = params - lr * grads\n",
        "        state = next_state\n",
        "\n",
        "print(\"Trained parameters:\", params)"
      ],
      "id": "s_zJ-5zDyI3r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr-Nz-WDyI3r"
      },
      "source": [
        "## 4. Evaluation"
      ],
      "id": "Jr-Nz-WDyI3r"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjSVpk5ByI3r"
      },
      "source": [
        "eval_episodes = 5\n",
        "for ep in range(eval_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    path = [state]\n",
        "    while not done:\n",
        "        action = select_action(state, params)\n",
        "        state, _, done = env.step(action)\n",
        "        path.append(state)\n",
        "    print(f\"Episode {ep+1} path: {path}\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(path, 'o-', markersize=12)\n",
        "    plt.xticks(range(env.size))\n",
        "    plt.yticks([])\n",
        "    plt.title(f\"Episode {ep+1} Path\")\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.show()"
      ],
      "id": "qjSVpk5ByI3r",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}